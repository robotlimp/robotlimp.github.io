<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Verifiably Following Complex Robot Instructions with Foundation Models.">
  <meta name="keywords"
    content="Limp, robotlimp, foundation models, robot instruction following, visual language models, motion planning, task and motion planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Verifiably Following Complex Robot Instructions with Foundation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-D5Q8X3K99Q"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-D5Q8X3K99Q');
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/spotlogo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Verifiably Following Complex Robot Instructions with Foundation
              Models</h1>
            <div class="is-size-4 publication-authors">
              <span class="author-block">
                <a href="https://benedictquartey.github.io/">Benedict Quartey*</a>,</span>
              <span class="author-block">
                <a href="https://eric-rosen.github.io/">Eric Rosen*</a>,</span>
              <span class="author-block">
                <a href="https://cs.brown.edu/people/stellex/">Stefanie Tellex</a>,
              </span>
              <span class="author-block">
                <a href="https://cs.brown.edu/people/gdk/">George Konidaris</a>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Brown University</span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/assets/supplementary.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-file-image fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-image" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M384 121.941V128H256V0h6.059a24 24 0 0 1 16.97 7.029l97.941 97.941a24.002 24.002 0 0 1 7.03 16.971zM248 160c-13.2 0-24-10.8-24-24V0H24C10.745 0 0 10.745 0 24v464c0 13.255 10.745 24 24 24h336c13.255 0 24-10.745 24-24V160H248zm-135.455 16c26.51 0 48 21.49 48 48s-21.49 48-48 48-48-21.49-48-48 21.491-48 48-48zm208 240h-256l.485-48.485L104.545 328c4.686-4.686 11.799-4.201 16.485.485L160.545 368 264.06 264.485c4.686-4.686 12.284-4.686 16.971 0L320.545 304v112z"></path></svg>
                    </span>
                    <span>Supplementary Materials</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
  </section>

  <!-- <section class="section" style="margin-top: -5%">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Abstract</h2>
        </div>
        <p class="subtitle has-text-justified" style="font-size: 14px; padding-top: 1%;">
          Enabling mobile robots to follow complex natural language instructions is a challenging problem. When instructing robots, users want to flexibly express constraints, refer to arbitrary landmarks, and verify robot behavior, while robots must disambiguate instructions into specifications and ground instruction referents in the real world. We introduce Language Instruction grounding for Motion Planning (LIMP), an approach that enables robots to verifiably follow complex, open-ended instructions in real-world environments without prebuilt semantic maps. LIMP constructs a symbolic instruction representation that reveals the robot's alignment with an instructor's intended motives and affords the synthesis of robot behaviors that are correct-by-construction. We conduct a large-scale evaluation of LIMP on 150 instructions across five real-world environments, demonstrating its versatility and ease of deployment in diverse, unstructured domains. LIMP performs comparably to state-of-the-art baselines on standard open-vocabulary tasks and additionally achieves a 79\% success rate on complex spatiotemporal instructions, significantly outperforming baselines, which only reach 38\%.
        </p>
      </div>
    </div>
  </section> -->


  <section style="margin-top: -3%">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img id="teaser" src="./static/images/limp_splash.jpg" alt="Limp teaser image." class="teaser-image"
          height="100%" />

        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/nav.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/pick.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/place.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/full.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <h2 class="subtitle has-text-justified" style="font-size: 14px; padding-top: 1%;">
          <!-- Language Instruction grounding for Motion Planning (LIMP) executing the instruction.  -->
          This figure visualizes our approach, Language Instruction grounding for Motion Planning (LIMP), executing the
          instruction: "<span style="font-style: italic;">Bring the green plush toy to the whiteboard in front of it,
            watch out for the
            robot in front of the toy</span>".
          LIMP has no semantic information of the environment prior to this instruction, rather at runtime, our approach
          leverages VLMs and spatial reasoning to detect and ground open-vocabulary instruction referents. LIMP then
          generates a verifiably correct task and motion plan that enables the robot to navigate from its start
          location <span style="color: goldenrod;">(yellow, A)</span>, to the green plush toy <span
            style="color: green;">(green, B)</span>, execute a pick skill which searches for and grasps
          the object, then navigate to the whiteboard <span style="color: blue;">(blue, C)</span> while avoiding the
          robot in the space <span style="color: red;">(red circles)</span>, to finally execute a place skill which sets
          the object down.
        </h2>
      </div>

    </div>
  </section>

  <div class="container">
    <div class="container is-max-desktop">
      <div class="column has-text-centered">
        <p class="title is-3">Real-World Demonstration<br><span class="title is-5">(All robot videos are 1x
            speed)</span></p>
        <p class="subtitle has-text-justified" style="font-size: 14px; padding-top:1%">
          Below are additional real world examples of LIMP generating task and motion plans to follow expressive
          instructions with complex spatiotemporal constraints.
          Each demonstration has two videos, the top video visualizes instruction translation, referent grounding,
          task progression semantic maps and computed motion plans for each example. The bottom video
          shows a robot executing the generated plan in the real world. Please see our paper for more details on our
          approach.
        </p>
      </div>
    </div>
    <br>

    <div class="columns is-centered">
      <div class="column has-text-centered">
        <div class="content">
          <h3 class="title is-5">Demo 1</h3>
          <!-- <video id="replay-video" controls muted preload loop playsinline width="100%">
            <source src="./static/videos/demo1_info.mp4" type="video/mp4">
          </video> -->
          <div class="publication-video">
            <iframe src="https://drive.google.com/file/d/1QLxN8P_aOznVyFZtBmpuHv5XCj2-kghS/preview" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
        <div class="content has-text-centered">
          <div class="publication-video">
            <video id="replay-video" controls muted preload loop playsinline width="100%">
              <source src="./static/videos/demo1.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>


      <div class="column has-text-centered">
        <div class="content">
          <h3 class="title is-5">Demo 2</h3>
          <!-- <video id="replay-video" controls muted preload loop playsinline width="100%">
            <source src="./static/videos/demo2_info.mp4" type="video/mp4">
          </video> -->
          <div class="publication-video">
            <iframe src="https://drive.google.com/file/d/1nsySPHRg1qYTNoJ4IqEJTbxndqa4T5By/preview" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
        <div class="content has-text-centered">
          <div class="publication-video">
            <video id="replay-video" controls muted preload loop playsinline width="100%">
              <source src="./static/videos/demo2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content">
          <h3 class="title is-5">Demo 3</h3>
          <!-- <video id="replay-video" controls muted preload loop playsinline width="100%">
            <source src="./static/videos/demo3_info.mp4" type="video/mp4">
          </video> -->
          <div class="publication-video">
            <iframe src="https://drive.google.com/file/d/1Pn81r4SPOu2V4_DBoIEhSg6G34iEUGds/preview" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
        <div class="content has-text-centered">
          <div class="publication-video">
            <video id="replay-video" controls muted preload loop playsinline width="100%">
              <source src="./static/videos/demo3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container" style="padding-top: 3%;">
    <div class="container is-max-desktop">
      <div class="column has-text-centered">
        <p class="title is-3">Baseline Comparison<br><span class="title is-5">(All robot videos are 1x
            speed)</span></p>
        <p class="subtitle has-text-justified" style="font-size: 14px; padding-top:1%">
          We compare LIMP with baseline implementations of an LLM task planner (NLmap-Saycan) and an LLM code-writing
          planner
          (Code-as-policies), representing
          state-of-the-art approaches for open-ended language-conditioned robot instruction following. To ensure
          competitive
          performance, we integrate our spatial grounding module and low-level robot control into these baselines,
          allowing them to query our module for 3D object positions, execute
          manipulation options, and use our path planner. We observe that LLM and code-writing planners are quite adept
          at generating
          sequential subgoals, but struggle with temporal constraint adherence. In contrast, our approach ensures each
          robot step adheres to constraints while achieving subgoals, as illustrated in the example below.
        </p>

        <p><span
            style="font-size: 14px; font-weight: bold; text-decoration: underline;">Instruction</span><br><span>"Hey, I
            want you to bring the plush toy on the table to the tree, </span> <span style="color: red;">make sure to
            avoid the trash bin when bringing the toy"</span></p>
        <br>
      </div>
    </div>


    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h3 class="title is-5">LIMP (Ours)</h3>
        <div class="content has-text-centered">
          <div class="publication-video">
            <video id="replay-video" controls muted preload loop playsinline width="100%">
              <source src="./static/videos/baselines/toy_tree_limp_planner.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>


      <div class="column has-text-centered">
        <div class="content has-text-centered">
          <h3 class="title is-5">NLMap-Saycan</h3>
          <div class="publication-video">
            <video id="replay-video" controls muted preload loop playsinline width="100%">
              <source src="./static/videos/baselines/toy_tree_LLM_Planner.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>

      <div class="column has-text-centered">
        <div class="content has-text-centered">
          <h3 class="title is-5">Code-as-policies</h3>
          <div class="publication-video">
            <video id="replay-video" controls muted preload loop playsinline width="100%">
              <source src="./static/videos/baselines/toy_tree_codegen_planner.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container" style="padding-top: 3%;">
    <div class="container is-max-desktop">
      <div class="column has-text-centered">
        <p class="title is-3">More Demonstration Videos</p>
        <p class="subtitle has-text-justified" style="font-size: 14px; padding-top:1%">
          We run a comprehensive evaluation on 150 natural language instructions in multiple real world environments.
          See below for some additional instruction following videos.
        </p>
        <select id="videoSelector" onchange="loadVideo()">
          <!-- <option value="" disabled selected>Select a video</option> -->
          <option value="./static/videos/extra_demos/toy_poster_seat.mp4">I am sitting on the brown sofa under the
            painting get me the plush toy on the table</option>
          <option value="./static/videos/extra_demos/book_printer.mp4">Bring the red book to the small printer on the
            right side of the big one</option>
          <option value="./static/videos/extra_demos/coke_to_crate.mp4">Help me pack the coke into the black crate
          </option>
          <option value="./static/videos/extra_demos/microwave_sink.mp4">I want to wash the red bottle in the sink, i
            think i left it in the microwave bring it</option>
          <option value="./static/videos/extra_demos/redbottle_computer.mp4">Go get the red bottle and bring it to the
            computer close to the green trash bin</option>
        </select>

        <!-- Video player -->
        <div id="videoContainer" style="margin-top: 20px;">
          <video id="videoPlayer" autoplay controls muted preload loop playsinline width="100%"
            style="border-radius: 15px;">
            <source id="videoSource" src="./static/videos/extra_demos/toy_poster_seat.mp4" type="video/mp4">
          </video>
        </div>
      </div>

    </div>
  </div>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="column has-text-centered">
        <h2 class="title">BibTeX</h2>
      </div>
      <pre><code>
        @article{quartey2024verifiably,
          title={Verifiably Following Complex Robot Instructions with Foundation Models},
          author={Quartey, Benedict and Rosen, Eric and Tellex, Stefanie and Konidaris, George},
          journal={arXiv preprint arXiv:2402.11498},
          year={2024}
        }</code></pre>
    </div>
    <div class="container is-max-desktop">
    <p> Special thanks to <a href="https://nerfies.github.io/">Nerfies</a> for an awesome website template!</p>
    </div>
  </section> -->

  <br>
  <br>

</body>

<script>
  function loadVideo() {
    var videoSelector = document.getElementById('videoSelector');
    var videoPlayer = document.getElementById('videoPlayer');
    var videoSource = document.getElementById('videoSource');

    var selectedVideo = videoSelector.value;
    videoSource.src = selectedVideo;
    videoPlayer.load();
  }
</script>

</html>